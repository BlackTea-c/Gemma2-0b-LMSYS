{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":9006294,"sourceType":"datasetVersion","datasetId":5425739}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# gemma-2 is available from transformers>=4.42.3\n!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-22T08:01:23.728642Z","iopub.execute_input":"2024-07-22T08:01:23.729066Z","iopub.status.idle":"2024-07-22T08:01:55.253691Z","shell.execute_reply.started":"2024-07-22T08:01:23.729020Z","shell.execute_reply":"2024-07-22T08:01:55.252536Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers>=4.42.3\n  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m648.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.42.3) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, transformers, peft\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 peft-0.11.1 transformers-4.42.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport copy\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    BitsAndBytesConfig,\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    Gemma2Config,\n    PreTrainedTokenizerBase, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom sklearn.metrics import log_loss, accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-22T08:01:55.255752Z","iopub.execute_input":"2024-07-22T08:01:55.256062Z","iopub.status.idle":"2024-07-22T08:02:15.089582Z","shell.execute_reply.started":"2024-07-22T08:01:55.256033Z","shell.execute_reply":"2024-07-22T08:02:15.088806Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-22 08:02:03.430783: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-22 08:02:03.430889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-22 08:02:03.557070: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    output_dir: str = \"output\"\n    checkpoint: str = \"/kaggle/input/checkpoint-5748/checkpoint-5748\"  # 4-bit quantized gemma-2-9b-instruct\n    max_length: int = 1024\n    n_splits: int = 5\n    fold_idx: int = 0\n    optim_type: str = \"adamw_8bit\"\n    per_device_train_batch_size: int = 2\n    gradient_accumulation_steps: int = 2  # global batch size is 8 \n    per_device_eval_batch_size: int = 8\n    n_epochs: int = 2\n    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n    lr: float = 2e-4\n    warmup_steps: int = 20\n    lora_r: int = 16\n    lora_alpha: float = lora_r * 2\n    lora_dropout: float = 0.05\n    lora_bias: str = \"none\"\n    \nconfig = Config()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:02:15.090910Z","iopub.execute_input":"2024-07-22T08:02:15.091606Z","iopub.status.idle":"2024-07-22T08:02:15.100038Z","shell.execute_reply.started":"2024-07-22T08:02:15.091556Z","shell.execute_reply":"2024-07-22T08:02:15.098996Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Training Arguments","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"output\",\n    overwrite_output_dir=True,\n    report_to=\"none\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    save_strategy=\"steps\",\n    save_steps=200,\n    optim=config.optim_type,\n    fp16=True,\n    learning_rate=config.lr,\n    warmup_steps=config.warmup_steps,\n    resume_from_checkpoint=\"/kaggle/input/checkpoint-5748/checkpoint-5748\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:02:15.101669Z","iopub.execute_input":"2024-07-22T08:02:15.102267Z","iopub.status.idle":"2024-07-22T08:02:15.177890Z","shell.execute_reply.started":"2024-07-22T08:02:15.102233Z","shell.execute_reply":"2024-07-22T08:02:15.177089Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### LoRA config","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:02:15.180045Z","iopub.execute_input":"2024-07-22T08:02:15.180319Z","iopub.status.idle":"2024-07-22T08:02:15.185519Z","shell.execute_reply.started":"2024-07-22T08:02:15.180296Z","shell.execute_reply":"2024-07-22T08:02:15.184638Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Instantiate the tokenizer & model","metadata":{}},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:02:15.186527Z","iopub.execute_input":"2024-07-22T08:02:15.186803Z","iopub.status.idle":"2024-07-22T08:02:16.178305Z","shell.execute_reply.started":"2024-07-22T08:02:15.186780Z","shell.execute_reply":"2024-07-22T08:02:16.177372Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = Gemma2ForSequenceClassification.from_pretrained(\n    config.checkpoint,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-22T08:02:16.179533Z","iopub.execute_input":"2024-07-22T08:02:16.179813Z","iopub.status.idle":"2024-07-22T08:03:00.179021Z","shell.execute_reply.started":"2024-07-22T08:02:16.179790Z","shell.execute_reply":"2024-07-22T08:03:00.178248Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25103344bb834851ac02eb852ab970f6"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000ab5e654894294be2cc4f44c2d5750"}},"metadata":{}},{"name":"stderr","text":"Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-9b-it-bnb-4bit and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): Gemma2ForSequenceClassification(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n        (layers): ModuleList(\n          (0-15): 16 x Gemma2DecoderLayer(\n            (self_attn): Gemma2SdpaAttention(\n              (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n              (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n              (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm()\n            (post_attention_layernorm): Gemma2RMSNorm()\n            (pre_feedforward_layernorm): Gemma2RMSNorm()\n            (post_feedforward_layernorm): Gemma2RMSNorm()\n          )\n          (16-41): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm()\n            (post_attention_layernorm): Gemma2RMSNorm()\n            (pre_feedforward_layernorm): Gemma2RMSNorm()\n            (post_feedforward_layernorm): Gemma2RMSNorm()\n          )\n        )\n        (norm): Gemma2RMSNorm()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=3, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=3, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-22T08:03:00.180369Z","iopub.execute_input":"2024-07-22T08:03:00.181199Z","iopub.status.idle":"2024-07-22T08:03:00.192573Z","shell.execute_reply.started":"2024-07-22T08:03:00.181159Z","shell.execute_reply":"2024-07-22T08:03:00.191665Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trainable params: 7,891,456 || all params: 9,249,608,192 || trainable%: 0.0853\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Instantiate the dataset","metadata":{}},{"cell_type":"code","source":"ds = Dataset.from_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:03:00.193884Z","iopub.execute_input":"2024-07-22T08:03:00.194280Z","iopub.status.idle":"2024-07-22T08:03:05.041352Z","shell.execute_reply.started":"2024-07-22T08:03:00.194248Z","shell.execute_reply":"2024-07-22T08:03:05.040580Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58ee6c6a4ed4d4398e808186f57fa38"}},"metadata":{}}]},{"cell_type":"code","source":"class CustomTokenizer:\n    def __init__(\n        self, \n        tokenizer: PreTrainedTokenizerBase, \n        max_length: int\n    ) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __call__(self, batch: dict) -> dict:\n        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n        labels=[]\n        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n            if a_win:\n                label = 0\n            elif b_win:\n                label = 1\n            else:\n                label = 2\n            labels.append(label)\n        return {**tokenized, \"labels\": labels}\n        \n    @staticmethod\n    def process_text(text: str) -> str:\n        return \" \".join(eval(text, {\"null\": \"\"}))","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:03:05.042308Z","iopub.execute_input":"2024-07-22T08:03:05.042572Z","iopub.status.idle":"2024-07-22T08:03:05.052146Z","shell.execute_reply.started":"2024-07-22T08:03:05.042549Z","shell.execute_reply":"2024-07-22T08:03:05.051160Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"encode = CustomTokenizer(tokenizer, max_length=config.max_length)\nds = ds.map(encode, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:03:05.053281Z","iopub.execute_input":"2024-07-22T08:03:05.053544Z","iopub.status.idle":"2024-07-22T08:04:03.728611Z","shell.execute_reply.started":"2024-07-22T08:03:05.053521Z","shell.execute_reply":"2024-07-22T08:04:03.727674Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/57477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1018a52d10c7402db57dc071ae778a60"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Compute metrics\n\nWe'll compute the log-loss used in LB and accuracy as a auxiliary metric.","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_preds: EvalPrediction) -> dict:\n    preds = eval_preds.predictions\n    labels = eval_preds.label_ids\n    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n    loss = log_loss(y_true=labels, y_pred=probs)\n    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n    return {\"acc\": acc, \"log_loss\": loss}","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:04:03.730041Z","iopub.execute_input":"2024-07-22T08:04:03.730706Z","iopub.status.idle":"2024-07-22T08:04:03.737466Z","shell.execute_reply.started":"2024-07-22T08:04:03.730668Z","shell.execute_reply":"2024-07-22T08:04:03.736566Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Split\n\nHere, train and eval is splitted according to their `id % 5`","metadata":{}},{"cell_type":"code","source":"folds = [\n    (\n        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n    ) \n    for fold_idx in range(config.n_splits)\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:04:03.738828Z","iopub.execute_input":"2024-07-22T08:04:03.739173Z","iopub.status.idle":"2024-07-22T08:04:04.340120Z","shell.execute_reply.started":"2024-07-22T08:04:03.739141Z","shell.execute_reply":"2024-07-22T08:04:04.338926Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_idx, eval_idx = folds[config.fold_idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:04:04.343140Z","iopub.execute_input":"2024-07-22T08:04:04.343435Z","iopub.status.idle":"2024-07-22T08:04:04.347697Z","shell.execute_reply.started":"2024-07-22T08:04:04.343409Z","shell.execute_reply":"2024-07-22T08:04:04.346818Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(len(train_idx),len(eval_idx))","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:04:04.348742Z","iopub.execute_input":"2024-07-22T08:04:04.348989Z","iopub.status.idle":"2024-07-22T08:04:04.357035Z","shell.execute_reply.started":"2024-07-22T08:04:04.348967Z","shell.execute_reply":"2024-07-22T08:04:04.356169Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"45981 11496\n","output_type":"stream"}]},{"cell_type":"code","source":"train_idx, eval_idx = folds[config.fold_idx]\n\ntrainer = Trainer(\n    args=training_args, \n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=ds.select(train_idx),\n    eval_dataset=ds.select(eval_idx),\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n)\ntrainer.train(resume_from_checkpoint=\"/kaggle/input/checkpoint-5748/checkpoint-5748\",eval_steps=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:04:04.358061Z","iopub.execute_input":"2024-07-22T08:04:04.358356Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11' max='22990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   11/22990 02:05 < 89:10:18, 0.07 it/s, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='405' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 405/1437 1:03:46 < 2:42:55, 0.11 it/s]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}}]}